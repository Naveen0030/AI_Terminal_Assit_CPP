# ğŸ¦™ Ollama C++ AI Terminal Assistant

A powerful, FREE, and completely local AI assistant built in C++ that runs entirely on your machine!

---

### Demo Highlights:

- ğŸš€ Instant Setup - From installation to first chat in under 2 minutes
- ğŸ’¬ Natural Conversations - Multi-turn programming discussions with context memory
- ğŸ¨ Beautiful Interface - Colored terminal output with emojis and clear formatting
- âš¡ Model Switching - Change AI models on-the-fly (/model command)
- ğŸ”§ Programming Help - Real-time code assistance, debugging, and explanations
- ğŸ“Š System Commands - Built-in tools for model management and status checking

---

## âœ¨ Features

### ğŸ¯ *Core Capabilities*
- ğŸ¤– *Multi-turn Conversations* - Maintains context throughout your session
- ğŸ¨ *Colored Terminal Interface* - Beautiful, professional-looking output
- ğŸ”„ *Model Management* - Switch between different AI models instantly  
- ğŸ’¾ *Conversation History* - View and manage your chat history
- ğŸ”§ *Programming Focus* - Optimized for coding help and technical discussions
- ğŸš€ *Cross-Platform* - Works on Linux, macOS, and Windows

### ğŸ’° *Why Choose This Over ChatGPT API?*
| Feature | Ollama C++ Assistant | ChatGPT API |
|---------|---------------------|-------------|
| *Cost* | ğŸŸ¢ *100% FREE* | ğŸ”´ Pay per token |
| *Privacy* | ğŸŸ¢ *Completely Local* | ğŸ”´ Data sent to OpenAI |
| *Internet* | ğŸŸ¢ *Works Offline* | ğŸ”´ Requires connection |
| *Speed* | ğŸŸ¢ *No Network Latency* | ğŸ”´ Network dependent |
| *Rate Limits* | ğŸŸ¢ *Unlimited Usage* | ğŸ”´ API rate limits |
| *Models* | ğŸŸ¢ *Multiple Models* | ğŸ”´ Limited to OpenAI models |

### ğŸ® *Interactive Commands*
- /help - Show all available commands
- /models - List installed AI models  
- /model - Switch to a different model
- /status - Check Ollama connection
- /clear - Clear conversation history
- /history - View conversation history
- /quit - Exit the application

---

## ğŸš€ Quick Start

### Prerequisites
- *Ollama* installed on your system
- *C++17* compatible compiler
- *libcurl* and *nlohmann-json* libraries

### 1ï¸âƒ£ Install Ollama (if not already installed)

bash
# Linux/macOS
curl -fsSL https://ollama.ai/install.sh | sh

# Windows (PowerShell)
iex (irm ollama.ai/install.ps1)

### 2ï¸âƒ£ Start Ollama & Install Models
bash
# Start Ollama server (keep this running)
ollama serve

# In another terminal, install models:
ollama pull llama3.2          # Recommended for general use
ollama pull codellama         # Best for programming tasks  
ollama pull phi3:mini         # Lightweight & fast


### 4ï¸âƒ£ Compile & Run

### ğŸš€ Setup Guide (Windows)

#### 1. âœ… Install [Ollama](https://ollama.com/download)
After installation, run it once:
bash
ollama run llama3

This downloads and runs the model locally via http://localhost:11434.

â„¹ Ollama must be running in the background before starting the assistant.
